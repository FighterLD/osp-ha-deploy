# This file can be used directly by 'phd', see 'build-all.sh' in this
# directory for how it can be invoked.  The only requirement is a list
# of nodes you'd like it to modify.
#
# The scope of each command-block is controlled by the preceeding
# 'target' line. 
#
# - target=all
#   The commands are executed on evey node provided
#
# - target=local
#   The commands are executed from the node hosting phd. When not
#   using phd, they should be run from some other independant host
#   (such as the puppet master)
#
# - target=$PHD_ENV_nodes{N}
#   The commands are executed on the Nth node provided.
#   For example, to run on only the first node would be target=$PHD_ENV_nodes1
#
# Tasks to be performed at this step include:
# - Tweaking the IP stack to allow nonlocal binding and adjusting keepalive timings
# - Configuring haproxy
# - Adding the virtual IPs to the cluster
# - Putting haproxy under the cluster's control

#################################
# Scenario Requirements Section #
#################################
= VARIABLES =

PHD_VAR_network_internal

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes: 1

######################
# Deployment Scripts #
######################
= SCRIPTS =

target=all
....
yum install -y haproxy
echo net.ipv4.ip_nonlocal_bind=1 >> /etc/sysctl.d/haproxy.conf
echo 1 > /proc/sys/net/ipv4/ip_nonlocal_bind
# the keepalive settings must be set in *ALL* hosts interacting with rabbitmq.
cat >/etc/sysctl.d/tcp_keepalive.conf << EOF
net.ipv4.tcp_keepalive_intvl = 1
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_time = 5
EOF
sysctl net.ipv4.tcp_keepalive_intvl=1
sysctl net.ipv4.tcp_keepalive_probes=5
sysctl net.ipv4.tcp_keepalive_time=5

# HA Proxy defaults

cat > /etc/haproxy/haproxy.cfg << EOF
global
    daemon
defaults
    mode tcp
    maxconn 10000
    timeout connect 2s
    timeout client 10s
    timeout server 10s
EOF

# Special case front-ends

cat >> /etc/haproxy/haproxy.cfg << EOF
frontend vip-db
    bind ${PHD_VAR_network_internal}.200:3306
    timeout client 90s
    default_backend db-vms-galera

frontend vip-qpid
    bind ${PHD_VAR_network_internal}.201:5672
    timeout client 120s
    default_backend qpid-vms

frontend vip-horizon
    bind ${PHD_VAR_network_internal}.208:80
    timeout client 180s
    cookie SERVERID insert indirect nocache
    default_backend horizon-vms

frontend vip-ceilometer
    bind ${PHD_VAR_network_internal}.211:8777
    timeout client 90s
    default_backend ceilometer-vms

frontend vip-rabbitmq
    option clitcpka
    bind ${PHD_VAR_network_internal}.213:5672
    timeout client 900m
    default_backend rabbitmq-vms
EOF

# nova-metadata needs "balance roundrobin" for frontend?
#db-vms-mariadb:58:3306:90s
mappings="
keystone-admin:202:64:35357
keystone-public:202:64:5000
glance-api:203:70:9191
glance-registry:203:70:9292
cinder:204:73:8776
swift:205:79:8080
neutron:206:82:9696
nova-vnc-novncproxy:207:85:6080
nova-vnc-xvpvncproxy:207:85:6081
nova-metadata:207:85:8775
nova-api:207:85:8774
horizon:x:85:80:108s
heat-cfn:209:91:8000
heat-cloudw:209:91:8004
heat-srv:209:91:8004
ceilometer:x:97:8777
"

for mapping in $mappings; do 
    server=$(echo $mapping | awk -F: '{print $1}' | awk -F- '{print $1}')
    service=$(echo $mapping | awk -F: '{print $1}')
    src=$(echo $mapping | awk -F: '{print $2}')
    target=$(echo $mapping | awk -F: '{print $3}')
    port=$(echo $mapping | awk -F: '{print $4}')
    timeout=$(echo $mapping | awk -F: '{print $5}')

    echo "Creating mapping for ${server} ${service}"

    if [ ${src} != x ]; then
	echo "frontend vip-${service}" >> /etc/haproxy/haproxy.cfg
	echo "    bind ${PHD_VAR_network_internal}.${src}:${port}" >> /etc/haproxy/haproxy.cfg
	echo "    default_backend ${service}-vms" >> /etc/haproxy/haproxy.cfg
    fi

    echo "backend ${service}-vms" >> /etc/haproxy/haproxy.cfg
    echo "    balance roundrobin" >> /etc/haproxy/haproxy.cfg
    if [ ! -z $timeout ]; then
	echo "    timeout server ${timeout}" >> /etc/haproxy/haproxy.cfg
    fi
    for count in 1 2 3; do
	echo "    server rhos6-${server}${count} ${PHD_VAR_network_internal}.$(( ${target} + ${count} - 1)):${port} check inter 1s" >> /etc/haproxy/haproxy.cfg
    done
done

# Special case back-ends

cat >> /etc/haproxy/haproxy.cfg << EOF
backend qpid-vms
# comment out 'stick-table' and add 'balance roundrobin' for A/A cluster mode in qpid
    stick-table type ip size 2
    stick on dst
    timeout server 120s
    server rhos6-qpid1 ${PHD_VAR_network_internal}.103:5672 check inter 1s
    server rhos6-qpid2 ${PHD_VAR_network_internal}.104:5672 check inter 1s
    server rhos6-qpid3 ${PHD_VAR_network_internal}.105:5672 check inter 1s

backend db-vms-galera
    option httpchk
    stick-table type ip size 2
    stick on dst
    timeout server 90s
    server rhos6-db1 ${PHD_VAR_network_internal}.58:3306 check inter 1s port 9200
    server rhos6-db2 ${PHD_VAR_network_internal}.59:3306 check inter 1s port 9200
    server rhos6-db3 ${PHD_VAR_network_internal}.60:3306 check inter 1s port 9200

backend rabbitmq-vms
    option srvtcpka
    balance roundrobin
    timeout server 900m
    server rhos6-rabbitmq1 ${PHD_VAR_network_internal}.61:5672 check inter 1s
    server rhos6-rabbitmq2 ${PHD_VAR_network_internal}.62:5672 check inter 1s
    server rhos6-rabbitmq3 ${PHD_VAR_network_internal}.63:5672 check inter 1s
EOF
....

target=$PHD_ENV_nodes1
....
pcs resource create lb-haproxy systemd:haproxy op monitor start-delay=10s --clone

offset=200
for section in db qpid keystone glance cinder swift neutron nova horizon heat x ceilometer x rabbitmq; do
    if [ $section != x ]; then 
	pcs resource create vip-${section} IPaddr2 ip=${PHD_VAR_network_internal}.${offset} nic=eth1
    fi
    offset=$(( $offset + 1 ))
done
....
