# This file can be used directly by 'phd', see 'build-all.sh' in this
# directory for how it can be invoked.  The only requirement is a list
# of nodes you'd like it to modify.
#
# The scope of each command-block is controlled by the preceeding
# 'target' line. 
#
# - target=all
#   The commands are executed on evey node provided
#
# - target=local
#   The commands are executed from the node hosting phd. When not
#   using phd, they should be run from some other independant host
#   (such as the puppet master)
#
# - target=$PHD_ENV_nodes{N}
#   The commands are executed on the Nth node provided.
#   For example, to run on only the first node would be target=$PHD_ENV_nodes1
#
# Tasks to be performed at this step include:

#################################
# Scenario Requirements Section #
#################################
= VARIABLES =

PHD_VAR_deployment
PHD_VAR_network_internal
PHD_VAR_env_configdir

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes: 1

######################
# Deployment Scripts #
######################
= SCRIPTS =

target=all
....
yum install -y openstack-keystone openstack-utils nfs-utils
....

target=all
....
if [ $PHD_VAR_deployment = segregated ]; then

    # In the segregated case, we need to mount /srv for any scenario that needs to use it
    # Unfortunately https://bugzilla.redhat.com/show_bug.cgi?id=1175005 prevents the mount from succeeding by default

    systemctl daemon-reload
    systemctl start rpcbind.service
    systemctl start rpc-statd.service
fi

# Now mount /srv so that we can use $PHD_VAR_env_configdir further down

if grep -q srv /etc/fstab; then 
    echo /srv is already mounted; 
else
    mkdir -p /srv
    echo "${PHD_VAR_network_internal}.1:/srv       /srv                    nfs     defaults,v3     0 0" >> /etc/fstab
    mount /srv
fi
....


target=all
....
mkdir -p $PHD_VAR_env_configdir
if [ ! -e $PHD_VAR_env_configdir/ks_admin_token ]; then
   openssl rand -hex 10 > $PHD_VAR_env_configdir/ks_admin_token
fi

export SERVICE_TOKEN=$(cat $PHD_VAR_env_configdir/ks_admin_token)

openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token $SERVICE_TOKEN

if [ $PHD_VAR_deployment = collapsed ]; then
   openstack-config --set /etc/keystone/keystone.conf DEFAULT rabbit_hosts  rhos6-node1:11211,rhos6-node2:11211,rhos6-node3:11211
else
    openstack-config --set /etc/keystone/keystone.conf DEFAULT rabbit_hosts rhos6-rabbitmq1:11211,rhos6-rabbitmq2:11211,rhos6-rabbitmq3:11211
fi
openstack-config --set /etc/keystone/keystone.conf DEFAULT rabbit_ha_queues true

# Define the API endpoints. Be careful with replacing vip-keystone and shell escapes.

openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_endpoint 'http://vip-keystone:%(admin_port)s/'
openstack-config --set /etc/keystone/keystone.conf DEFAULT public_endpoint 'http://vip-keystone:%(public_port)s/'

# Configure access to galera. Note that several entries in here are dependent on
# what has been configured before. 'keystone' user, 'keystonetest' password, 
# vip-mysql.

openstack-config --set /etc/keystone/keystone.conf database connection mysql://keystone:keystonetest@vip-mysql/keystone

# Mare sure to retry connection to the DB if the DB is not available immediately at 
# service startup.

openstack-config --set /etc/keystone/keystone.conf database max_retries -1

# Make sure the API service is listening on the internal IP addresses only.
# Once again those shell expansions only work for my specific environment.

openstack-config --set /etc/keystone/keystone.conf DEFAULT public_bind_host $(ip addr show dev eth1 scope global | grep dynamic| sed -e 's#.*inet ##g' -e 's#/.*##g')
openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_bind_host $(ip addr show dev eth1 scope global | grep dynamic| sed -e 's#.*inet ##g' -e 's#/.*##g')

if [ ! -e $PHD_VAR_env_configdir/keystone_ssl.tar ]; then
    keystone-manage pki_setup --keystone-user keystone --keystone-group keystone
    cd /etc/keystone/ssl
    tar cvp -f $PHD_VAR_env_configdir/keystone_ssl.tar *
fi

mkdir -p /etc/keystone/ssl
cd /etc/keystone/ssl
tar xvp -f ${PHD_VAR_env_configdir}/keystone_ssl.tar
chown -R keystone:keystone /var/log/keystone /etc/keystone/ssl/
....


target=$PHD_ENV_nodes1
....
su keystone -s /bin/sh -c "keystone-manage db_sync"

pcs resource create keystone systemd:openstack-keystone --clone

if [ $PHD_VAR_deployment = collapsed ]; then

    # In a collapsed environment, we can instruct the cluster to start
    # things in a particular order and require services to be active
    # on the same hosts.  We do this with constraints.

    pcs constraint order start lb-haproxy-clone then keystone-clone
    pcs constraint order start galera-master then keystone-clone
    pcs constraint order start rabbitmq-server-clone then keystone-clone
    pcs constraint order start memcached-clone then keystone-clone
fi
....


target=$PHD_ENV_nodes1
....
export SERVICE_TOKEN=$(cat ${PHD_VAR_env_configdir}/ks_admin_token)
export SERVICE_ENDPOINT="http://vip-keystone:35357/v2.0"

keystone service-create --name=keystone --type=identity --description="Keystone Identity Service"

keystone endpoint-create --service keystone --publicurl 'http://vip-keystone:5000/v2.0' --adminurl 'http://vip-keystone:35357/v2.0' --internalurl 'http://vip-keystone:5000/v2.0'

keystone user-create --name admin --pass keystonetest
keystone role-create --name admin
keystone tenant-create --name admin
keystone user-role-add --user admin --role admin --tenant admin

# Save admin credential in a file. This will be useful many times over the how-to!

cat >  ${PHD_VAR_env_configdir}/keystonerc_admin << EOF
export OS_USERNAME=admin 
export OS_TENANT_NAME=admin
export OS_PASSWORD=keystonetest
export OS_AUTH_URL=http://vip-keystone:35357/v2.0/
export PS1='[\u@\h \W(keystone_admin)]\$ '
EOF

keystone user-create --name foo --pass footest
keystone role-create --name Member
keystone tenant-create --name TENANT
keystone user-role-add --user foo --role Member --tenant TENANT

# Save user credential in a file for testing purposes.

cat >  ${PHD_VAR_env_configdir}/keystonerc_user << EOF
export OS_USERNAME=foo
export OS_TENANT_NAME=TENANT
export OS_PASSWORD=footest
export OS_AUTH_URL=http://vip-keystone:5000/v2.0/
export PS1='[\u@\h \W(keystone_user)]\$ '
EOF
keystone tenant-create --name services --description "Services Tenant"

# glance
keystone user-create --name glance --pass glancetest
keystone user-role-add --user glance --role admin --tenant services
keystone service-create --name glance --type image --description "Glance Image Service"
keystone endpoint-create --service glance --publicurl "http://vip-glance:9292" --adminurl "http://vip-glance:9292" --internalurl "http://vip-glance:9292"

# cinder
keystone user-create --name cinder --pass cindertest
keystone user-role-add --user cinder --role admin --tenant services
keystone service-create --name cinder --type volume --description "Cinder Volume Service"
keystone endpoint-create --service cinder --publicurl "http://vip-cinder:8776/v1/\$(tenant_id)s" --adminurl "http://vip-cinder:8776/v1/\$(tenant_id)s" --internalurl "http://vip-cinder:8776/v1/\$(tenant_id)s"

# swift
keystone user-create --name swift --pass swifttest
keystone user-role-add --user swift --role admin --tenant services
keystone service-create --name swift --type object-store --description "Swift Storage Service"
keystone endpoint-create --service swift --publicurl "http://vip-swift:8080/v1/AUTH_\$(tenant_id)s" --adminurl "http://vip-swift:8080/v1" --internalurl "http://vip-swift:8080/v1/AUTH_\$(tenant_id)s"

# neutron
keystone user-create --name neutron --pass neutrontest
keystone user-role-add --user neutron --role admin --tenant services
keystone service-create --name neutron --type network --description "OpenStack Networking Service"
keystone endpoint-create --service neutron --publicurl "http://vip-neutron:9696" --adminurl "http://vip-neutron:9696" --internalurl "http://vip-neutron:9696"

# nova
keystone user-create --name compute --pass novatest
keystone user-role-add --user compute --role admin --tenant services
keystone service-create --name compute --type compute --description "OpenStack Compute Service"
keystone endpoint-create  --service compute --publicurl "http://vip-nova:8774/v2/\$(tenant_id)s" --adminurl "http://vip-nova:8774/v2/\$(tenant_id)s" --internalurl "http://vip-nova:8774/v2/\$(tenant_id)s"

# heat
keystone user-create --name=heat --pass=heattest
keystone user-role-add --user heat --role admin --tenant services
keystone service-create --name heat --type orchestration
keystone endpoint-create --service heat --publicurl "http://vip-heat:8004/v1/%(tenant_id)s" --adminurl "http://vip-heat:8004/v1/%(tenant_id)s" --internalurl "http://vip-heat:8004/v1/%(tenant_id)s"
keystone service-create --name heat-cfn --type cloudformation
keystone endpoint-create --service heat-cfn --publicurl "http://vip-heat:8000/v1" --adminurl "http://vip-heat:8000/v1" --internalurl "http://vip-heat:8000/v1"

# ceilometer
keystone user-create --name ceilometer --pass ceilometertest --email fdinitto@redhat.com
keystone user-role-add --user ceilometer --role admin --tenant services
keystone role-create --name ResellerAdmin
keystone user-role-add --user ceilometer --role ResellerAdmin --tenant services
keystone service-create --name ceilometer --type metering --description="OpenStack Telemetry Service"
keystone endpoint-create --service ceilometer --publicurl "http://vip-ceilometer:8777" --adminurl "http://vip-ceilometer:8777" --internalurl "http://vip-ceilometer:8777"
....
