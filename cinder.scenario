# This file can be used directly by 'phd', see 'build-all.sh' in this
# directory for how it can be invoked.  The only requirement is a list
# of nodes you'd like it to modify.
#
# The scope of each command-block is controlled by the preceeding
# 'target' line. 
#
# - target=all
#   The commands are executed on evey node provided
#
# - target=local
#   The commands are executed from the node hosting phd. When not
#   using phd, they should be run from some other independant host
#   (such as the puppet master)
#
# - target=$PHD_ENV_nodes{N}
#   The commands are executed on the Nth node provided.
#   For example, to run on only the first node would be target=$PHD_ENV_nodes1
#
# Tasks to be performed at this step include:

#################################
# Scenario Requirements Section #
#################################
= VARIABLES =

PHD_VAR_network_internal
PHD_VAR_rpm_osp

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes: 1

######################
# Deployment Scripts #
######################
= SCRIPTS =

target=all
....
yum install -y openstack-cinder openstack-utils python-memcached nfs-utils python-keystonemiddleware
....


target=all
....
openstack-config --set /etc/cinder/cinder.conf database connection mysql://cinder:cindertest@vip-mysql/cinder
openstack-config --set /etc/cinder/cinder.conf database max_retries -1

openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_host vip-keystone
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_tenant_name services
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_user cinder
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken admin_password cindertest

openstack-config --set /etc/cinder/cinder.conf DEFAULT notification_driver messaging
openstack-config --set /etc/cinder/cinder.conf DEFAULT control_exchange cinder
openstack-config --set /etc/cinder/cinder.conf DEFAULT rabbit_host vip-rabbitmq

openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_host vip-glance

openstack-config  --set /etc/cinder/cinder.conf DEFAULT memcache_servers  rhos6-memcache1:11211,rhos6-memcache2:11211,rhos6-memcache3:11211

openstack-config --set /etc/cinder/cinder.conf DEFAULT host rhos6-cinder

openstack-config --set /etc/cinder/cinder.conf DEFAULT osapi_volume_listen $(ip addr show dev eth1 scope global | grep dynamic| sed -e 's#.*inet ##g' -e 's#/.*##g')

# NOTE: this config section is to enable and configure the NFS cinder driver.

configdir=/srv/rhos-${PHD_VAR_rpm_osp}/cinder
# create the NFS share mountpoint on the nfs server
echo ssh ${PHD_VAR_network_internal}.1 -- mkdir -p $configdir

cat > /etc/cinder/nfs_exports << EOF
${PHD_VAR_network_internal}.1:$configdir
EOF

chown root:cinder /etc/cinder/nfs_exports
chmod 0640 /etc/cinder/nfs_exports

openstack-config --set /etc/cinder/cinder.conf DEFAULT nfs_shares_config /etc/cinder/nfs_exports
openstack-config --set /etc/cinder/cinder.conf DEFAULT nfs_sparsed_volumes true
openstack-config --set /etc/cinder/cinder.conf DEFAULT nfs_mount_options v3

openstack-config --set /etc/cinder/cinder.conf DEFAULT volume_driver cinder.volume.drivers.nfs.NfsDriver
....


target=$PHD_ENV_nodes1
....
su cinder -s /bin/sh -c "cinder-manage db sync"

# create services in pacemaker
pcs resource create cinder-api systemd:openstack-cinder-api --clone
pcs resource create cinder-scheduler systemd:openstack-cinder-scheduler --clone

# Volume must be A/P for now. See https://bugzilla.redhat.com/show_bug.cgi?id=1193229
pcs resource create cinder-volume systemd:openstack-cinder-volume

pcs constraint order start cinder-api-clone then cinder-scheduler-clone
pcs constraint colocation add cinder-scheduler-clone with cinder-api-clone
pcs constraint order start cinder-scheduler-clone then cinder-volume
pcs constraint colocation add cinder-volume with cinder-scheduler-clone
....

