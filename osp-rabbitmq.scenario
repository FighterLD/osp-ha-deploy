# This file can be used directly by 'phd', see 'build-all.sh' in this
# directory for how it can be invoked.  The only requirement is a list
# of nodes you'd like it to modify.
#
# The scope of each command-block is controlled by the preceeding
# 'target' line. 
#
# - target=all
#   The commands are executed on evey node provided
#
# - target=local
#   The commands are executed from the node hosting phd. When not
#   using phd, they should be run from some other independant host
#   (such as the puppet master)
#
# - target=$PHD_ENV_nodes{N}
#   The commands are executed on the Nth node provided.
#   For example, to run on only the first node would be target=$PHD_ENV_nodes1
#
# Tasks to be performed at this step include:

#################################
# Scenario Requirements Section #
#################################
= VARIABLES =

# Expands to $PHD_VAR_network_domain, $PHD_VAR_network_internal, etc
network:
  domain: lab.bos.redhat.com
  internal: 192.168.124

rpm:
  osp: 6.0

#################################
# Scenario Requirements Section #
#################################
= REQUIREMENTS =
nodes: 1

######################
# Deployment Scripts #
######################
= SCRIPTS =

target=all
....
# You may need to reboot after installing nfs-utils
yum install -y rabbitmq-server nfs-utils

# NOTE: we need to bind the service to the internal IP address

cat > /etc/rabbitmq/rabbitmq-env.conf << EOF
NODE_IP_ADDRESS=$(ip addr show dev eth1 scope global | grep dynamic| sed -e 's#.*inet ##g' -e 's#/.*##g')
EOF

# required to generate the cookies
systemctl start rabbitmq-server
systemctl stop rabbitmq-server
....

target=$PHD_ENV_nodes1
....
if grep -q srv /etc/fstab; then 
    echo /srv is already mounted; 
else
    mkdir -p /srv
    echo "${PHD_VAR_network_internal}.1:/srv       /srv                    nfs     defaults,v3     0 0" >> /etc/fstab
    mount /srv
fi
mkdir -p /srv/rhos-${PHD_VAR_rpm_osp}/configs/
cp /var/lib/rabbitmq/.erlang.cookie /srv/rhos-${PHD_VAR_rpm_osp}/configs/rabbitmq_erlang_cookie
....

target=all
....
if grep -q srv /etc/fstab; then 
    echo /srv is already mounted; 
else
    mkdir -p /srv
    echo "${PHD_VAR_network_internal}.1:/srv       /srv                    nfs     defaults,v3     0 0" >> /etc/fstab
    mount /srv
fi

# the cookie has to be the same across all nodes. Copy around as preferred, I am 
# using my NFS commodity storage. Also check for file permission/ownership. I 
# workaround that step by using 'cat' vs cp.
cat /srv/rhos-${PHD_VAR_rpm_osp}/configs/rabbitmq_erlang_cookie > /var/lib/rabbitmq/.erlang.cookie
....


target=$PHD_ENV_nodes1
....
pcs resource create rabbitmq-server systemd:rabbitmq-server --clone
....

target=all
....
# all nodes except the first must be stopped
if [ $PHD_ENV_nodes1 == $(uname -n).vmnet.${PHD_VAR_network_domain} ]; then
    rabbitmqctl stop_app
fi
....

target=all
....
# make all nodes except node1 join in series
if [ $PHD_ENV_nodes1 != $(uname -n).vmnet.${PHD_VAR_network_domain} ]; then
    short=$(echo $PHD_ENV_nodes1 | awk -F. '{print $1}')
    rabbitmqctl join_cluster rabbit@$short
fi
....

target=all
....
# everyone has joined, now they can be restarted 
if [ $PHD_ENV_nodes1 != $(uname -n).vmnet.${PHD_VAR_network_domain} ]; then
    rabbitmqctl start_app
fi
....

target=$PHD_ENV_nodes1
....
rabbitmqctl set_policy HA '^(?!amq\.).*' '{"ha-mode": "all"}'
....
